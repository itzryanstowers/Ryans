

000000000000000 REQUESTS 000000000000000

# IMPORT REQUESTS LIBRARY
import requests

# DECLARE REQUEST URLS
bitcoin_price_url = "https://api.coindesk.com/v1/bpi/currentprice.json"


# ENSURING THE FORMAT IS JSON
bitcoin_price_url = bitcoin_price_url + '?format=json'

# EXECUTE GET REQUESTS USING REQUESTS LIBRARY
response_data = requests.get(bitcoin_price_url)


#GET RESPONSE STATUS CODE
requests.get(response_data)

#EXTRACT RESPONSE CONTENT
bitcoin_data = requests.get(bitcoin_price_url)
bitcoin_content = bitcoin_data.content

#CONVERT OUTPUT TO JSON. ENSURE JSON IMPORTED
data = bitcoin_data.json()


# USE 'json.dumps' TO FORMAT DATA
json.dumps(data, indent = 4)

# SELECT VALUE AND STORE AS VARIABLE 
btc_value = data['bpi']['USD']['rate']






000000000000000 LIBRARIES 000000000000000

import pandas as __
import numpy as __
from pathlib import Path
#!pip install quandl
import quandl
%matplotlib inline
import seaborn as __
import json __






000000000000000 SET WEIGHTS WITH DOT PRODUCT 000000000000000



# Shortcut for calculating returns
weights = [0.5, 0.5]
portfolio_returns = all_returns.dot(weights)        #dot product

#usually we would use a function that outputs a portfolio...a list of weights that are assigned to a given stock... 
# then we simply dot product the two

portfolio_returns.head()


# Calculate Portfolio Returns with an equal amount of each stock
amd_weight = 0.5
mu_weight = 0.5


#WE INVEST HALF OF OUR MONEY IN BOTH AMD AND MU... each are weighted 2 and they make up all of our portfolio
portfolio_returns = amd_weight * all_returns["AMD"] + mu_weight * all_returns["MU"]
portfolio_returns.head()




# Higher Volalitity Portfolio (More risk, but potentially higher returns)
initial_investment = 10000
weights = [0.8, 0.2]                                           
portfolio_returns = all_returns.dot(weights)                   # all of the stock percent changes times how much of each stock we have... weight
cumulative_returns = (1 + portfolio_returns).cumprod()          #reinvesting proceeds and adding one dollar
(initial_investment * cumulative_returns).plot()                # how our initial investment grows over time




000000000000000 PIVOT TABLE 000000000000000



# Create a new pivot table where the columns are the closing prices for each ticker
all_prices = pd.concat([amd, mu], axis= "rows", join="inner")
all_prices = all_prices.reset_index()
all_prices

# THIS IS PIVOTINR THE DATA AND MAKING IS PRETTY
all_prices = all_prices.pivot_table(values="NOCP", index="Trade DATE", columns="Symbol")








000000000000000 COVARIANCE, VARIANCE, BETA 000000000000000

# USE PCT_CHANGE TO CALC DAILY RETURNS OF CLOSE FOR EACH COLUMN
daily_returns = combined_df.pct_change()

# CALCULATE COVARIANCE OF AMZN RETURNS VS S&P RETURNS
covariance = daily_returns['AMZN'].cov(daily_returns["S&P 500"])

# CALCULATE VARIANCE OF S&P RETURNS
variance_mkt = daily_returns["S&P 500"].var()

# CALCULATE BETA OF AMZN
amzn_beta = covariance / variance_mkt


# LINEAR MODEL USING 'seaborn'
sns.lmplot(x='x_variable_column', y='y_variable_column')






000000000000000 ROLLING STATISTICS 000000000000000

# PLOT 7-DAY ROLLING MEAN OF TSLA CLOSE PRICE
tsla_df.rolling(window=7).mean().plot()





# PLOT 7-DAY STD OF TSLA CLOSE PRICE
# NOTICE WE CHANGED 'window=7' to '7'
tsla_df.rolling(7).std().plot()



# OVERLAY DAILY CLOSE AND 180-DAY ROLLING MEAN OF TSLA
# 1) SET FIGURE OF THE DAILY CLOSING PRICES OF TESLA
ax = tsla_df.plot()

# 2) PLOT 180-DAY ROLLING MEAN ON THE SAME FIGURE
tsla_df.rolling(window=180).mean().plot(ax=ax)

# 3) SET THE LEGEND OF THE FIGURE
ax.legend(["TSLA", "TSLA 180 DAY MEAN"])









000000000000000 CORRELATION 000000000000000


# NEW LIBRARY NEEDED IS 'seaborn'

# CALCULATE CORRELATION BETWEEN EACH COLUMN
correlation = combined_df.corr()

# HEAT MAP WITH CORRELATIONS USING 'seaborn' LIBRARY
# 'vmin=-1, vmax=1' MEANS WE FIX THE CORRELATION BETWEEN VALUES OF -1 AND 1
sns.heatmap(correlation, vmin=-1, vmax=1)






000000000000000 MULTIPLE INDEXING 000000000000000


# DISPLAY DATA-FRAME INDEX: 
my_data.index

# CREATE MULTIPLE INDICES BY GROUPING BY DATE-TIME-INDEX YEAR, MONTH, AND DAY:
# USE '.first()' WHICH GRABS FIRST VALUE OF EACH GROUP
my_data_grp = my_data.groupby([my_data.index.year, my_data.index.month, my_data.index.day]).first()

# CREATE MULTIPLE INDICES GROUP BY YEAR AND MONTH
# THEN GRAB LAST VALUE OF EACH GROUP:
my_data_grp_3 = my_data.groupby([my_data.index.year, my_data.index.month]).last()

# CREATE MULTIPLE INDICES BY GROUPING BY DATE-TIME-INDEX YEAR AND MONTH
# THEN GET THE MEAN FUNCTION:
my_data_grp_4 = my_data.groupby([my_data.index.year, my_data.index.month]).mean()

# SLICE(MEANING ACCESS THAT ELEMENT/SET OF ELEMENTS) DATA FOR 4/12/2019:
my_data_slice = my_data_grp.loc[2019,4,12]

# SLICE DATA FOR ALL DAYS IN APRIL 2019:
my_data_slice = my_data_grp.loc[2019,4]

# RENAME INDICIES:
goog_df_grp






000000000000000 DATA FRAME CONCATENATION 000000000000000


# CONCATENATE DATA BY ROWS USING CONCAT FUNCTION AND INNER JOIN
# 'axis=0' MEANS STACK ON TOP, JOIN THEM ROW-WISE. 'axis=1' MEANS JOIN COLUMN-WISE
joined_data_rows = pd.concat([france_data, uk_data, netherlands_data], axis=0, join="inner")







000000000000000 PANDAS VISUALIZATIONS 000000000000000


# IMPORT LIBRARIES 
import pandas as pd
from pathlib import Path
%matplotlib inline

# PLOT LINE CHART WITH NO INDEX
my_csv.plot()

# SET DATE-TIME INDEX, DROP EXTRA COLUMNS, RE-PLOT
my_csv.set_index(pd.to_datetime(my_csv['Date'], infer_datetime_format=True), inplace=True)
my_csv.drop(columns=['Date'], inplace=True)
my_csv.plot()

# PLOT BAR CHART
my_csv.plot(kind='bar')

# PLOT BAR WITH INCREASED FIGURE SIZE
my_csv.plot(kind='bar', figsize=(20,10))

# PLOT BAR WITH TITLE
my_csv.plot(kind='bar', title = 'cool title')

# PLOT PIE CHART
sector_count.plot(kind='pie')

# PLOT SCATTER-PLOT RELATIONSHIP BETWEEN PRICE AND EPS
my_csv.plot(kind='scatter', x='Earnings/Share', y='Price')

# PLOT BAR CHART OF SHARPE RATIOS USING '.plot.bar'
sharpe_ratios.plot.bar(title='Sharpe Ratios')

# PLOT STANDARD DEVIATION WITH BOX PLOT
portfolio_std.plot.box()





000000000000000 SORTING/GROUPING 000000000000000


# SORT ASCENDING (DEFAULT)
my_df.sort_values("Price")

# SORT DESCENDING
my_df.sort_values("Price", ascending=False)

# SORT BY CLOSE IN DESCENDING ORDER
tsla_top_changes = tsla_daily_returns.sort_values("Close", ascending=False)

# GROUP DATA BY 'metric'
crypto_data_grp = crypto_data.groupby('metric')

# SINGLE COLUMN GROUPING
# GROUP BY 'cryptocurrency'
# THEN COUNT HOW MANY 'price' WE HAVE PER EACH 'cryptocurrency'
single_group = crypto_data.groupby('cryptocurrency')['price'].count()

# GROUP BY MULTIPLE COLUMNS
# BELOW WE GROUP BY 'cryptocurrency' AND 'price'...
# THEN COUNT 'price'... SO COUNT EACH INSTANCE OF EACH PRICE GIVEN EACH CURRENCY
multi_group = crypto_data.groupby(['cryptocurrency','price'])['price'].count()

# ROUND ALL ROWS IN COLUMN TO x PLACES
rounded_crypto_data = crypto_data.round({'price': x})






000000000000000 INDEXING IN DATA FRAMES 000000000000000


# SELECT FIRST ROW OF DATA FRAME
my_csv.iloc[0]

# SELECT FIRST 10 ROWS OF DATA FRAME
my_csv.iloc[0:10] 

# SELECT LAST ROW
my_csv.iloc[-1]

# SELECT FIRST COLUMN, SECOND COLUMN, LAST COLUMN, RESPECTIVELY
my_csv.iloc[:,0]
my_csv.iloc[:,1]
my_csv.iloc[:,-1]

# SELECT FIRST 2 COLUMNS WITH ALL ROWS
my_csv.iloc[:, 0:2].head()

# SELECT THE 1st, 5th, 8th, 22nd ROWS OF THE 1st 4th AND 6th COLUMNS.
my_csv.iloc[[0,4,7,21], [0,3,5]]

# SELECT FIRST 5 ROWS OF 3RD, 4TH, AND 5TH COLUMNS
my_csv.iloc[0:5, 2:5] 

# CHANGE 'first_name' COLUMN VALUE OF THE FIRST ROW TO 'Jimmie'
my_csv.iloc[0, my_csv.columns.get_loc('first_name')] = 'Jimmie'

# SET THE INDEX TO 'column 1'... 'inplace=True' MAKES OUR DF SAVE THIS CHANGE 
my_csv.set_index(my_csv['column 1'], inplace=True)

# SORT THE INDEX AND RESAVE DATA FRAME WITH 'inplace=True'
people_csv.sort_index(inplace=True)

# FILTER ROWS BASED ON COLUMN VALUE CONDITIONAL
people_csv.loc[people_csv['gender'] == 'M'].head()

# MODIFY THE 'first_name' VALUE OF THE ROW WITH INDEX 'Yun'
people_csv.loc['Yun', 'first_name'] = 'Yuna'

# USING THE '.copy()' FUNCTION TO PRESERVE FORMAT OF 'filtered_df'
term_df = filtered_df.loc[filtered_df['term'] == '36 months'].copy()

# SHOW SUMMARY STATISTICS OF COLUMNS WHERE 'annual_inc' IS GREATER THAN 80000
term_df.loc[term_df['annual_inc'] > 80000].describe()

# SLICE DATA FRAME INTO 1 YEAR TIMEFRAME
daily_return_1_year = daily_return.loc['2018-04-30':'2019-04-29']






000000000000000 DATA VISUALIZATION, MANIPULATION, CLEANING 000000000000000


# GENERATE SUMMARY STATISTICS
my_df.describe()

# GENERATE SUMMARY STATISTICS FOR ALL COLUMNS
csv.describe(include='all')

# GET DATA TYPES
csv_data.dtypes

# COUNT ROWS
csv_data.count()

# ID NUMBER OF ROWS AND COLUMNS
csv_data.shape

# ID NUMBER OF TIMES A VALUE OCCURS
csv_data["customer_no"].value_counts()

# CALCULATE MAX OR MIN PRICE OF 'crypto' FOR EACH TYPE
crypto_data.groupby('crypto')['priceUsd'].max() OR .min()

# NULL CHECK
csv_data.isnull()

# GET PERCENTAGE OF NULLS
csv_data.isnull().mean() * 100

# GET NUMBER OF NULLS
csv_data.isnull().sum()

# FILL NULLS WITH DEFAULT VALUE (i.e. "Unknown", 0, or mean() is common)
csv_data["customer_no"] = csv_data["customer_no"].fillna("Unknown")

# DROP NULLS
csv_data = csv_data.dropna().copy()

# DUPLICATE CHECK
csv_data.duplicated()

# DUPLICATE CHECK WITHIN SPECIFIC FIELDS
csv_data["customer_no"].duplicated()

# DROP DUPLICATES
csv_data = csv_data.drop_duplicates().copy()

# BOOLEAN DUPLICATE CHECK FOR GIVEN FIELD
csv_data["customer_no"].duplicated()

# REPLACE/REMOVE CURRENCY SYMBOLS
csv_data["order_total"] = csv_data["order_total"].str.replace("$", "")

# CONVERT OBJECT FROM ONE VARIABLE TYPE TO FLOAT
csv_data["order_total"] = csv_data["order_total"].astype("float")





000000000000000 DATA FRAME COLUMN MANIPULATION 000000000000000


# RENAME COLUMN
my_df_grp.rename(columns={"Close': 'closing_price'})

# RENAME COLUMNS
my_df = my_df.rename(columns={
    "Old name1": "New Name1",
    "Old name2": "New Name2"
})

# REORDER COLUMNS
my_df = my_df[['newOrder1', 'New Order 2']]

# CREATE COLUMNS
my_df["new column"] = my_df["old column that will be kept"] / 419.99

# SPLIT COLUMNS (SPLITTING AN EMAIL ADDRESS AT THE "@" SYMBOL.
split_email_df = my_df["emailAddress"].str.split("@", expand=True) 	# split_email_df IS NOW A NEW DATA FRAME

# SETTING NAMES TO THE NEWLY ADDED COLUMNS
my_df["New name for added column 1"] = split_email_df[0]
my_df["New name for added column 2"] = split_email_df[1]

# DELETE COLUMNS
my_df = my_df.drop(columns=["old column we'll remove"])

# REWRITE COLUMN NAMES
columns = ["Full Name", "Email", "Address"]
my_df.columns = columns





000000000000000 CSV - DATA FRAMES 000000000000000


# READ DATE-TIME CSV. INDEX BECOMES DATE. INFER DATE TIME FORMAT. PARSE DATES INTO COLUMNS[M, Y, DAY].
csv_path = Path("../Resources/twtr_google_finance.csv")
my_data = pd.read_csv(csv_path, parse_dates=True, index_col='Date', infer_datetime_format=True)

# IMPORT PANDAS FOR CSV MANIPULATION
import pandas as pd
from pathlib import Path

# CREATE A PATH TO FILE
csvpath = Path("file/location.csv")

# READ CSV INTO A DATAFRAME USING PANDAS
my_df = pd.read_csv(csvpath)

# CHECK FIRST FEW ROWS OF OUR DATA
my_df.head()

# SETTING HEADER TO NONE TO AVIOD HAVING AN ELEMENT AS THE TITLE
our_data = pd.read_csv(csvpath, header=None)

# PRINT FIRST 10 AND LAST 2 ROWS FROM OUR DATA FRAME
print(my_df.head(10))
print(my_df.tail(2))

# IMPORT NECESSARY LIBRARIES FOR READING CSV FILES
import os
import csv

# SET THE PATH FOR THE CSV FILE
csvpath = os.path.join("..", "Resources", "pokemon.csv")

# OPEN THE CSV IN A DIFFERENT WAY... WITH '.reader'
with open(csvpath, newline="") as csvfile:
    csvreader = csv.reader(csvfile, delimiter=",")

    # ITERATE THROUGH THE DATA AND SEARCH FOR THE NUMBER THE USER INPUTTED. REMEMBER TO SKIP THE HEADER OF THE CSV.
    csv_header = next(csvreader)

    for row in csvreader:
        # PRINT THE NAME OF THE POKEMON (IDENTIFIER) AND POKEDEX NUMBER (SPECIES ID) AT THAT NUMBER. FOR EXAMPLE, "Pokemon No. 25 - Pikachu".
        print(f"Pokemon No. {row[0]} - {row[1]}")

        # ITERATE THROUGH THE DATA AND SEARCH FOR POKEMON WHOSE WEIGHT IS GREATER THAN 3000. APPEND ONLY THE POKEMON'S NAME AND WEIGHT TO THE 'heaviest' LIST.
        if int(row[4]) > 3000:
            heaviest.append(row[1])

        # ITERATE THROUGH THE DATA AND SEARCH FOR POKEMON WHOSE HEIGHT IS GREATER THAN 50. APPEND ONLY THE POKEMON'S NAME AND HEIGHT TO THE 'tallest' LIST.
        if int(row[3]) > 50:
            tallest.append(row[1])





000000000000000 BASIC FUNCTIONS 000000000000000


# DEFINE A FUNCTION "warble" THAT TAKES IN A STRING AS AN ARGUMENT, ADDS " arglebargle" TO THE END OF IT, AND RETURNS THE RESULT.
def warble(string_param):
    return f"{string_param} arglebargle"

# PRINT THE RESULT OF CALLING YOUR "warble" FUNCTION WITH THE ARGUMENT "hello".
print(warble("hello"))

# DEFINE A FUNCTION "wibble" THAT TAKES A STRING AS AN ARGUMENT, PRINTS THE ARGUMENT, PREPENDS "wibbly " TO THE ARGUMENT, AND RETURNS THE RESULT.
def wibble(string_param):
    return f"wibbly {string_param}"

# PRINT THE RESULT OF CALLING YOUR "wibble" FUNCTION WITH THE ARGUMENT "bibbly"
print(wibble("bibbly"))

# DEFINE A FUNCTION "print_sum" THAT TAKES IN TWO NUMBERS AS ARGUMENTS AND PRINTS THE SUM OF THOSE TWO NUMBERS.
def print_sum(num_1, num_2):
    sum = num_1 + num_2
    print(sum)

# DEFINE A FUNCTION "return_sum" THAT TAKES IN TWO NUMBERS AS ARGUMENTS AND RETURNS THE SUM OF THOSE TWO NUMBERS.
def return_sum(num_1, num_2):
    sum = num_1 + num_2
    return sum

# DEFINE A FUNCTION "triple_sum" THAT TAKES IN 3 ARGUMENTS AND RETURNS THE SUM OF THOSE 3 NUMBERS.
def triple_sum(num_1, num_2, num_3):
    sum = num_1 + num_2 + num_3
    return sum

# DEFINE A FUNCTION "dance_party" THAT TAKES IN A STRING AS AN ARGUMENT, PRINTS "dance!", UPDATES THE STRING FROM CALLING "wibble" FUNCTION WITH THAT ARGUMENT, UPDATES THE STRING FROM CALLING "warble" FUNCTION WITH THAT ARGUMENT, RETURNS THE UPDATED STRING.
def dance_party(string_param):
    print("dance!")
    print(string_param)
    print(wibble(warble(string_param)))

# PRINT THE RESULT OF CALLING YOUR "dance_party" FUNCTION WITH YOUR NAME AS THE ARGUMENT
dance_party("Andrew")


# DEFINE A FUNCTION `having_fun` THAT PRINTS "Functions are FUN!".
def having_fun():
    print("Functions are FUN!")

# DEFINE A FUNCTION `thirty_seven` THAT PRINTS THE SUM OF 18 AND 19.
def thirty_seven():
    sum = 18 + 19
    print(sum)

# CALL YOUR `user_input` FUNCTION.
user_input()

# DEFINE A FUNCTION `hello` THAT TAKES IN A STRING PARAMETER AND PRINTS THE PARAMETER VARIABLE.
def hello(string_param):
    print(string_param)

# CALL YOUR `hello` FUNCTION.
hello("Hello World!")

# DEFINE A FUNCTION `user_input` THAT ASKS THE USER "What is your name?" AND STORES IT IN A VARIABLE CALLED `user_name` AND PRINT THE USER'S NAME.
def user_input():
    user_name = input("What is your name? ")
    print(user_name)

# DEFINE A FUNCTION `average` THAT CALCULATES THE AVERAGE BETWEEN TWO PARAMETERS AND RETURNS THE AVERAGE.
def average(num_1, num_2):
    avg_calc = num_1 + num_2 / 2
    return avg_calc

# CALL THE `average` FUNCTION AND ASSIGN TO A VARIABLE `calculated_average`.
calculated_average = average(90, 10)
print(f"Calculated Average: {calculated_average}")






000000000000000 ITERATION THROUGH LISTS 000000000000000


# ITERATE THROUGH THE PROVIDED `num_list` AND CREATE AN IF-ELSE STATEMENT TO PRINT EVERY NUMBER GREATER THAN 50
for number in num_list:
    if number > 50:
        print(number)

# ITERATE THROUGH THE PROVIDED `num_list` AND USE THE `index` FUNCTION TO PRINT THE INDEX OF THE FIRST OCCURRENCE OF THE NUMBER 11
index = 0
for number in num_list:
    if number == 11:
        print(index)
    index += 1

# ITERATE THROUGH THE PROVIDED `num_list` AND PRINT THE SUM OF ALL THE NUMBERS
sum = 0
for number in num_list:
    sum += number
print(sum)

# ITERATE THROUGH THE PROVIDED `num_list` AND CREATE AN IF-ELSE STATEMENT TO PRINT THE SUM OF ALL THE NUMBERS GREATER THAN 50
sum = 0
for number in num_list:
    if number > 50:
        sum += number
print(sum)

# ITERATE THROUGH THE PROVIDED `num_list` AND CREATE AN IF-ELSE STATEMENT TO PRINT THE SUM OF ALL THE EVEN NUMBERS
sum = 0
for number in num_list:
    if number % 2 == 0:
        sum += number
print(sum)

# ITERATE THROUGH THE PROVIDED `fruits` LIST AND PRINT THE NUMBER OF TIMES "Apple" APPEARS IN THE LIST
fruits = [
  "Apple", "Orange", "Banana", "Pomelo", "Apple", "Kiwi", "Peach", "Banana", "Grape", "Tomato",
  "Kiwi", "Apple", "Watermelon", "Lemon", "Pomelo", "Apple", "Banana", "Peach", "Apricot", "Grape"]

count = 0
for fruit in fruits:
    if fruit == "Apple":
        count += 1
print(count)

# ITERATE THROUGH THE PROVIDED `fruits` LIST AND PRINT THE NUMBER OF TIMES "Peach" APPEARS IN THE LIST
count = 0
for fruit in fruits:
    if fruit == "Peach":
        count += 1
print(count)

# ITERATE THROUGH THE PROVIDED `fruits` LIST AND PRINT THE NUMBER OF FRUITS THAT START WITH "P" IN THE LIST
count = 0
for fruit in fruits:
    if fruit[0] == "p" or fruit[0] == "P":
        count += 1
print(count)

# "NOT IN" FUNCTION WITH FOR-LOOPS
for fruit in fruits:
    if fruit not in unique_fruits:
        unique_fruits.append(fruit)






000000000000000 NESTED LIST EXAMPLE 000000000000000


# YOU CAN STORE ANY TYPE OF DATA WITHIN A LIST- EVEN OTHER LISTS!
two_dim_list = [
  [54, 6, 7, 46, 78],
  [43, 9, 6, 65, 65],
  [32, 1, 44, 1, 23],
  [55, 12, 2, 34, 2],
  [2, 12, 44, 2, 12]]

# ITERATE THROUGH THE FIRST LIST INSIDE `two_dim_list` AND PRINT ALL THE NUMBERS LESS THAN 25
for number in two_dim_list[0]:
    if number < 25:
        print(number)

# ITERATE THROUGH THE SECOND LIST INSIDE `two_dim_list` AND PRINT ALL THE NUMBERS LESS THAN 25
for number in two_dim_list[1]:
    if number < 25:
        print(number)

# ITERATE THROUGH THE FIFTH LIST INSIDE `two_dim_list` AND PRINT ALL THE NUMBERS LESS THAN 25
for number in two_dim_list[4]:
    if number < 25:
        print(number)

# ITERATE THROUGH `two_dim_list` AND THE LISTS INSIDE OF IT AND PRINT ALL THE ODD NUMBERS
for one_dim_list in two_dim_list:
    for number in one_dim_list:
        if number % 2 != 0:
            print(number)

# ITERATE THROUGH `two_dim_list` AND THE LISTS INSIDE OF IT AND PRINT THE SUM OF ALL THE NUMBERS THAT ARE A MULTIPLE OF 3
sum = 0
for one_dim_list in two_dim_list:
    for number in one_dim_list:
        if number % 3 == 0:
            sum += number
print(sum)






000000000000000 MATHEMATICAL FUNCTIONS 000000000000000



# CALCULATE DAILY RETURNS WITH '.shift()' 
daily_returns = (sp500_csv - sp500_csv.shift(1)) / sp500_csv.shift(1)

# CALCULATE DAILY RETURNS WITH '.pct_change()' 
daily_returns = sp500_csv.pct_change()

# CALCULATE CUMULATIVE RETURNS WITH '.comprod()'
cumulative_returns = (1 + daily_returns).cumprod()

# CALCULATE ANNUALIZED STANDARD DEVIATION FOR EACH ELEMENT IN 'my_returns'
my_std_annual = my_returns.std() * np.sqrt(252)

# CALCULATE ANNUALIZED STANDARD DEVIATION FOR EACH ELEMENT IN 'my_returns'
my_sharpe_ratios = (my_returns.mean() * 252) / (my_std_annual)

# 


000000000000000 DICTIONARIES 000000000000000


# BASIC DICTIONARY CREATION
san_francisco = {
    "east_coast": False,
    "has_multiple_bridges": True,
    "state": "California",
    "sports_teams": ["Giants", "Warriors", "Forty-Niners"],
    "population": 884363,
    "natural_disasters": ["Earthquakes"],
}

# INDEXING INTO A DICTIONARY
# ACCESS IMDB LINK FROM DICTIONARY OF TV SHOWS WE USED IN CLASS
print(shows['genre']['comedy']['family_guy']['cast'][4]['imdb'])






000000000000000 LISTS 000000000000000


# CREATE LIST WITH ELEMENTS
farm = ["pig", "cow", "chicken", "dog", "horse", "sheep"]

# PRINT FIRST ELEMENT OF LIST
print(list1[0])

# COMBINE LISTS 
list_combined = list1 + list2

# APPEND ELEMENT TO END OF LIST
my_list.append(1) OR my_list.append(False) OR my_list.append("hello") OR my_list.append






000000000000000 CONDITIONALS 000000000000000


# BASIC CONDITIONAL 
if variable1 == variable2:
    print("equal")
elif variable1 > variable2:
    print("not equal, 1 bigger")
else:
    print("not equal, 2 bigger")

# CONDITIONAL WITH "AND" OPERATION 
if x == y and y == z:
    print("x = z")
else:
    print('not equal')

# WRITE AN IF STATEMENT THAT PRINTS THE STRING "RWAR!" IF THE FIRST ELEMENT OF farm IS NOT "Godzilla".
# AN ELSE IF STATEMENT THAT PRINTS THE STRING "SCREECH!" IF THE LAST ELEMENT OF farm IS "Mothra".
# ELSE, PRINT THE STRING "This animal is neither Godzilla nor Mothra!".
if farm[0] == "Godzilla":
    print("RWAR!")
elif farm[-1] == "Mothra":
    print("SCREECH!")
else:
    print("This animal is neither Godzilla nor Mothra!")






000000000000000 INTRODUCTION TO VARIABLES 000000000000000


# DECLARE VARIABLE AS AN INPUT
variable1 = input("Type your text for variable1")

# CREATE A VARIABLE NAMED 'SUBJECT' WITH NO VALUE (NONE).
subject = None

# CREATE A VARIABLE, 'PROFESSION', AND ASSIGN IT A VALUE OF A STRING, "COMPUTER PROGRAMMER".
profession = "Computer Programmer"

# CREATE A VARIABLE, 'FIRST_NAME', AND ASSIGN IT A VALUE OF AN EMPTY STRING.
first_name = ""

# CREATE A VARIABLE, 'FULL_NAME', AND ASSIGN IT A VALUE OF THE COMBINATION OF 'FIRST_NAME' AND 'LAST_NAME' WITH A SPACE.
full_name = f"{first_name} {last_name}"

# CREATE A VARIABLE, 'BIRTH_YEAR', AND ASSIGN IT WITH AN INTEGER OF 1815.
birth_year = 1815

# CREATE A VARIABLE, 'AGE_AT_PASSING', AND ASSIGN IT A VALUE OF 'DEATH_YEAR' MINUS 'BIRTH_YEAR'.
age_at_passing = death_year - birth_year

# PRINT: "FIRST NAME: " AND 'FIRST_NAME'.
print(f"First Name: {first_name}")

# PRINT: "LAST NAME: " AND 'LAST_NAME'.
print(f"Last Name: {last_name}")

# PRINT: "PROFESSION: " AND 'PROFESSION'.
print(f"Profession: {profession}")

# PRINT: "BIRTHYEAR: " AND 'BIRTH_YEAR'.
print(f"Birth Year: {birth_year}")






000000000000000 TERMINAL MANIPULATION 000000000000000


# LIST CONTENTS OF CURRENT DIRECTORY
ls

# PRINT CURRENT DIRECTORY
pwd

# CREATE FOLDER "folder1"
mkdir folder1

# CREATE TEXT FILE "text1"
touch text1.txt

# COPY "text1" FROM "folder1" INTO "folder2"
cp folder1/text1.txt folder2

# RENAME FOLDERS
mv text1.txt newtext.txt

# READ FILE IN TERMINAL
cat text1.txt

# REMOVE FILE
rm -r file1


000000000000000 Finance Calculations 000000000000000
Correlation:
Measures the statistical relationship between two variables. A correlation close to 1 indicates a strong positive relationship, while a correlation close to -1 indicates a strong negative relationship. 
Closer to 1: Indicates a strong positive relationship.
Closer to -1: Indicates a strong negative relationship.
Near 0: Indicates a weak or no linear relationship.

Standard Deviation (std):
Measures the amount of variation or dispersion of a set of values. In finance, it is often used as a measure of volatility. A higher standard deviation implies higher volatility.
Lower values: Indicate lower volatility and, in some cases, may be considered more favorable for stability.
Higher values: Indicate higher volatility, which might be less favorable if stability is a priority.

Beta:
Indicates the sensitivity of an asset's returns to changes in the returns of a benchmark (usually the market, represented by an index like the S&P 500). A beta greater than 1 implies higher volatility compared to the benchmark.
Beta = 1: The asset's returns move in line with the benchmark (market).
Beta > 1: Indicates higher volatility than the benchmark.
Beta < 1: Indicates lower volatility than the benchmark.

Cumulative Return:
Represents the total return of an investment over a specified time period, considering both capital appreciation and dividends.
Higher values: Indicate higher overall returns over the specified time period.

Sharpe Ratio:
Measures the risk-adjusted performance of an investment. It considers both the returns and the volatility of the investment. A higher Sharpe ratio indicates better risk-adjusted performance.
Rolling Exponential Moving Average
Higher values: Indicate better risk-adjusted performance. A Sharpe ratio above 1 is often considered good, and higher values are generally more favorable.

(ewa):
Provides a smoothed representation of time-series data by giving more weight to recent observations. Useful for identifying trends over time.
Rolling Standard Deviation

(rolling_std):
Similar to standard deviation but calculated over a rolling window. It helps identify changes in volatility over time.

Interpretation depends on the specific analysis and goals. Generally, lower volatility (rolling_std) and alignment with the upward trend (ewa) may be considered favorable.